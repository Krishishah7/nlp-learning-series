# Transformers â€” NLP Learning Series

This folder contains notebooks related to transformer-based models and their internal working in Natural Language Processing (NLP).

The notebooks focus on understanding how transformers process text, starting from tokenization and moving towards embeddings, attention mechanisms, and feature extraction.

## Topics Covered

- Transformer tokenization (subword / WordPiece)
- Input representations and token IDs
- Attention masks and special tokens
- Transformer-based embeddings
- Feature extraction using pretrained models

## Purpose

Transformers form the foundation of modern NLP systems such as BERT, GPT, and sentence transformers.  
Understanding their internal components helps in building advanced applications like semantic search, question answering, and retrieval-augmented generation (RAG).

## Note

These notebooks are part of a long-term learning series and will be updated incrementally with additional transformer-related experiments.
